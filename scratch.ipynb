{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import hypernetx as hnx\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random tensor\n",
    "n_nodes = 100\n",
    "m = 3\n",
    "K = 3\n",
    "dim = [K] * m\n",
    "\n",
    "# Randomly assign each node to a cluster\n",
    "clusters = np.random.randint(0,K,n_nodes)\n",
    "\n",
    "# One-hot encoding of the clusters\n",
    "one_hot = np.zeros((n_nodes,K))\n",
    "for i in range(n_nodes):\n",
    "    one_hot[i,clusters[i]] = 1\n",
    "one_hot = torch.tensor(one_hot)\n",
    "\n",
    "P_hat = torch.zeros(dim)\n",
    "for i in range(K):\n",
    "    for j in range(i,K):\n",
    "        for k in range(j,K):\n",
    "            # Random variable uniformly distributed between 0 and 1\n",
    "            p = np.random.uniform()\n",
    "            P_hat[i,j,k] = p\n",
    "            P_hat[i,k,j] = p\n",
    "            P_hat[j,i,k] = p\n",
    "            P_hat[j,k,i] = p\n",
    "            P_hat[k,i,j] = p\n",
    "            P_hat[k,j,i] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n",
      "torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "# Print the size of P_hat and one_hot\n",
    "print(P_hat.size())\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.1741, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.1768, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1690],\n",
       "        [0.0000, 0.0000, 0.1690]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize each column of one_hot\n",
    "one_hot = one_hot.float()\n",
    "one_hot = one_hot / (one_hot.sum(0) ** 0.5)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31.,  0.,  0.],\n",
       "        [ 0., 37.,  0.],\n",
       "        [ 0.,  0., 32.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if one_hot has orthogonal columns\n",
    "one_hot.T @ one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('pytorch')\n",
    "P_hat = P_hat.float()\n",
    "one_hot = one_hot.float()\n",
    "Q = tl.tucker_to_tensor((P_hat, [one_hot,one_hot,one_hot]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "def n_mode_product(x, u, n):\n",
    "    n = int(n)\n",
    "    # We need one letter per dimension\n",
    "    # (maybe you could find a workaround for this limitation)\n",
    "    if n > 26:\n",
    "        raise ValueError('n is too large.')\n",
    "    ind = ''.join(chr(ord('a') + i) for i in range(n))\n",
    "    exp = f'{ind}K...,JK->{ind}J...'\n",
    "    result = tf.einsum(exp, x, u)\n",
    "    return torch.tensor(result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_mode_dot(tensor, matrices, modes):\n",
    "    res = tensor\n",
    "    for mode, matrix in zip(modes, matrices):\n",
    "        res = n_mode_product(res, matrix, mode)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold(tensor, mode):\n",
    "    \n",
    "    return torch.reshape(torch.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOOI(tensor, ranks, n_iter_max=1000, tol=1e-8):\n",
    "    \"\"\"High-Order Orthogonal Iteration (HOOI) algorithm for Tucker decomposition\n",
    "    Perform Higher Order Orthogonal Iteration (HOOI) on a 3D tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: Input tensor of size I x J x K.\n",
    "    - ranks: Tuple of target ranks (r1, r2, r3).\n",
    "    - n_iter_max: Maximum number of iterations.\n",
    "    - eps: Convergence criterion.\n",
    "\n",
    "    Returns:\n",
    "    - core_tensor: Core tensor of the Tucker decomposition.\n",
    "    - [L, R, V] factor matrices.\n",
    "    L ∈ R^I×r1, R ∈ R^J×r2, V ∈ R^K×r3.\n",
    "    \"\"\"\n",
    "    # Initialize R and V factor matrices with orthonormal columns\n",
    "    I, J, K = tensor.shape\n",
    "    R = ortho_group.rvs(J)[:,:ranks[1]]\n",
    "    R_hat = torch.tensor(R, dtype=torch.float32)\n",
    "    V = ortho_group.rvs(K)[:,:ranks[2]]\n",
    "    V_hat = torch.tensor(V, dtype=torch.float32)\n",
    "\n",
    "    for _ in range(n_iter_max):\n",
    "        # C = A ×2 R^T ×3 V^T\n",
    "        c = multi_mode_dot(tensor, [R_hat.T, V_hat.T], modes=[1, 2])\n",
    "        c_1 = unfold(c, 0)\n",
    "\n",
    "        # L = SVD(r1, C_1), where U = SVD(k, C) means compute the k’th order truncated \n",
    "        # SVD of C and then set U = [u1, u2, . . . , uk] to the matrix whose columns are \n",
    "        # the k largest left singular vectors ui of C\n",
    "        u, _, _ = torch.svd(c_1)\n",
    "        L_hat = u[:, :ranks[0]]\n",
    "\n",
    "        # D = A ×1 L^T ×3 V^T\n",
    "        d = multi_mode_dot(tensor, [L_hat.T, V_hat.T], modes=[0, 2])\n",
    "        d_2 = unfold(d, 1)\n",
    "\n",
    "        #R = SVD(r2, D_2)\n",
    "        u, _, _ = torch.svd(d_2)\n",
    "        R_hat = u[:, :ranks[1]]\n",
    "\n",
    "        # E = A ×1 L^T ×2 R^T\n",
    "        e = multi_mode_dot(tensor, [L_hat.T, R_hat.T], modes=[0, 1])\n",
    "        e_3 = unfold(e, 2)\n",
    "\n",
    "        # V = SVD(r3, E_3)\n",
    "        u, _, _ = torch.svd(e_3)\n",
    "        V_hat = u[:, :ranks[2]]\n",
    "\n",
    "        # Compute the approximation error\n",
    "        core_tensor = n_mode_product(e, V_hat.T, 2)\n",
    "        appr = multi_mode_dot(core_tensor, [L_hat, R_hat, V_hat], modes=[0, 1, 2])\n",
    "        if torch.norm(tensor - appr) < tol:\n",
    "            break\n",
    "\n",
    "    return core_tensor, [L_hat, R_hat, V_hat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Tucker decomposition to find the factor matrices\n",
    "core, factors = HOOI(Q, ranks=[K,K,K], n_iter_max=10000, tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_invariant(matrix):\n",
    "    \"\"\"Scale-invariant tensor, proposed the following row-wise normalization on given matrix\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : torch tensor\n",
    "    Returns\n",
    "    -------\n",
    "    torch tensor\n",
    "        scale-invariant matrix\n",
    "    \"\"\"\n",
    "    # For each row, divide by the first coordinate of the row\n",
    "    return matrix / matrix[:, 0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# The adjacency tensor must be symmetric, then the factor matrices are same, so we can use the first one\n",
    "factor = factors[0]\n",
    "\n",
    "# Apply scale-invariant to the factor matrix\n",
    "factor = scale_invariant(factor)\n",
    "R_hat = factor[:, 1:]\n",
    "\n",
    "# Apply K-mean Clustering to the rows of factor matrix\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(R_hat)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 2, 0, 1, 1, 2, 1, 1, 2, 2, 0, 0, 2, 0, 1, 2, 1, 0, 2,\n",
       "       0, 2, 2, 2, 1, 0, 1, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 0, 2, 1, 2, 1,\n",
       "       0, 2, 0, 0, 2, 2, 1, 2, 1, 0, 0, 0, 1, 2, 2, 1, 2, 0, 2, 0, 1, 1,\n",
       "       2, 2, 0, 1, 2, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 2, 2, 1, 2,\n",
       "       2, 0, 1, 0, 1, 2, 1, 0, 0, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all the indices whose label is 0\n",
    "indices_0 = [i for i, x in enumerate(labels) if x == 0]\n",
    "\n",
    "# Take all the indices whose label is 1\n",
    "indices_1 = [i for i, x in enumerate(labels) if x == 1]\n",
    "\n",
    "# Take all the indices whose label is 2\n",
    "indices_2 = [i for i, x in enumerate(labels) if x == 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 2, 1, 0, 0, 2, 0, 0, 2, 2, 1, 1, 2, 1, 0, 2, 0, 1, 2,\n",
       "       1, 2, 2, 2, 0, 1, 0, 1, 1, 2, 2, 1, 2, 0, 1, 1, 1, 1, 2, 0, 2, 0,\n",
       "       1, 2, 1, 1, 2, 2, 0, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2, 1, 2, 1, 0, 0,\n",
       "       2, 2, 1, 0, 2, 0, 2, 2, 2, 1, 1, 0, 1, 0, 2, 1, 1, 0, 2, 2, 0, 2,\n",
       "       2, 1, 0, 1, 0, 2, 0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all the indices whose cluster is 2\n",
    "clusters_0 = [i for i, x in enumerate(clusters) if x == 1]\n",
    "\n",
    "# Take all the indices whose cluster is 0\n",
    "clusters_1 = [i for i, x in enumerate(clusters) if x == 0]\n",
    "\n",
    "# Take all the indices whose cluster is 1\n",
    "clusters_2 = [i for i, x in enumerate(clusters) if x == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Check how many same indices are in the indices_0 and clusters_0\n",
    "print(len(set(indices_0) & set(clusters_0)) / len(set(indices_0) | set(clusters_0)))\n",
    "\n",
    "# Check how many same indices are in the indices_1 and clusters_1\n",
    "print(len(set(indices_1) & set(clusters_1)) / len(set(indices_1) | set(clusters_1)))\n",
    "\n",
    "# Check how many same indices are in the indices_2 and clusters_2\n",
    "print(len(set(indices_2) & set(clusters_2)) / len(set(indices_2) | set(clusters_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(labels == clusters) / n_nodes\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = multi_mode_dot(core, factors, modes=[0, 1, 2])\n",
    "torch.norm(Q - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2383e-07)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(factors[0] - factors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8658e-07)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(factors[0] - factors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5468)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(factors[0] - one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000, -2.3583, -0.1519],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.5180, -1.4584],\n",
       "        [ 1.0000,  0.3714,  0.8176],\n",
       "        [ 1.0000,  0.3714,  0.8176]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(533.0045)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(core - P_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
